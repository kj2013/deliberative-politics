{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@author: Kokil\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#!pip install spacy\n",
    "#!python3 -m spacy download en_core_web_lg \n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "#!pip install convokit\n",
    "from convokit import Corpus, Speaker, Utterance\n",
    "from convokit import download\n",
    "from convokit import TextParser\n",
    "from convokit import PolitenessStrategies\n",
    "import spacy\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def is_number(tok):\n",
    "    try:\n",
    "        float(tok)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    return [tok.text if not is_number(tok.text) else '_NUM_' for tok in nlp(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_harbingers(df, X_col):\n",
    "\n",
    "    with open('/home/kokil/feature_extraction/data/2015_Diplomacy_lexicon.json') as f:\n",
    "        features = json.loads(f.readline())\n",
    "\n",
    "    for feature in features:\n",
    "        harbingers = [harbinger.encode('ascii', 'ignore').decode('ascii').lower() for harbinger in features[feature]]\n",
    "        features[feature] = harbingers\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = str(text)\n",
    "        text = text.replace('\\'', '')\n",
    "        text = text.lower()\n",
    "        text = text.replace('{html}',\"\") \n",
    "        text = re.sub(re.compile('<.*?>'), '', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub('[0-9]+', '', text)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)  \n",
    "        text = \" \".join(tokens)\n",
    "        return text\n",
    "\n",
    "    def get_feature_frequency(text, feature):\n",
    "        count = 0\n",
    "        for harbinger in features[feature]:\n",
    "            count += text.count(harbinger)\n",
    "        return count\n",
    "\n",
    "    df['clean_text'] = df.apply(lambda row: clean_text(row[X_col]), axis=1)\n",
    "    for feature in features:\n",
    "        df[feature] = df.apply(lambda row: get_feature_frequency(row['clean_text'], feature), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nlp = en_core_web_sm.load()\n",
    "#spacy.load(\"en_core_web_lg\")\n",
    "#spacy.load(\"en_core_web_sm\")\n",
    "ps = PolitenessStrategies()\n",
    "spacy_nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "cols = list(ps.transform_utterance(\"hello, could you please help me proofread this article?\", spacy_nlp=spacy_nlp).meta['politeness_strategies'])\n",
    "\n",
    "def extract_politeness_feats(df, X_col):\n",
    "\n",
    "    def extract_politeness_helper(row):\n",
    "        utt = ps.transform_utterance(row[X_col], spacy_nlp=spacy_nlp)\n",
    "        feats = [utt.meta['politeness_strategies'][x] for x in cols]\n",
    "        return pd.Series(feats)\n",
    "\n",
    "    df[cols] = df.apply(extract_politeness_helper, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List harbingers, liwc and politeness features\n",
    "import json\n",
    "with open('/home/kokil/feature_extraction/data/2015_Diplomacy_lexicon.json') as f:\n",
    "    harb_dict = json.loads(f.readline())\n",
    "politeness_dict = pd.read_csv('/home/kokil/feature_extraction/data/politeness_list.csv')\n",
    "liwc_dict = pd.read_csv('/home/kokil/feature_extraction/data/liwc_list.csv')\n",
    "X_cols = list(politeness_dict.columns) + list(liwc_dict.columns) + list(harb_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "happierfuntokenizer_v3\n",
    "\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\n",
    "The __main__ method illustrates by tokenizing a few examples.\n",
    "\n",
    "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
    "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
    "and Twitter is cooperating, then it should tokenize a random\n",
    "English-language tweet.\n",
    "\"\"\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "import html.entities\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8>]                    # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpPxX/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpPxX/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8<]                    # eyes\n",
    "      [<>]?\n",
    "      |\n",
    "      <[/\\\\]?3                         # heart(added: has)\n",
    "      |\n",
    "      \\(?\\(?\\#?                   #left cheeck\n",
    "      [>\\-\\^\\*\\+o\\~]              #left eye\n",
    "      [\\_\\.\\|oO\\,]                #nose\n",
    "      [<\\-\\^\\*\\+o\\~]              #right eye\n",
    "      [\\#\\;]?\\)?\\)?               #right cheek\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # http:\n",
    "    # Web Address:\n",
    "    r\"\"\"(?:(?:http[s]?\\:\\/\\/)?(?:[\\w\\_\\-]+\\.)+(?:com|net|gov|edu|info|org|ly|be|gl|co|gs|pr|me|cc|us|gd|nl|ws|am|im|fm|kr|to|jp|sg)(?:\\/[\\s\\b$])?)\"\"\"\n",
    "    ,\n",
    "    r\"\"\"(?:http[s]?\\:\\/\\/)\"\"\"   #need to capture it alone sometimes\n",
    "    ,\n",
    "    #command in parens:\n",
    "    r\"\"\"(?:\\[[\\w_]+\\])\"\"\"   #need to capture it alone sometimes\n",
    "    ,\n",
    "    # HTTP GET Info\n",
    "    r\"\"\"(?:\\/\\w+\\?(?:\\;?\\w+\\=\\w+)+)\"\"\"\n",
    "    ,\n",
    "    # HTML tags:\n",
    "    r\"\"\"(?:<[^>]+\\w=[^>]+>|<[^>]+\\s\\/>|<[^>\\s]+>?|<?[^<\\s]+>)\"\"\"\n",
    "    #r\"\"\"(?:<[^>]+\\w+[^>]+>|<[^>\\s]+>?|<?[^<\\s]+>)\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[\\w][\\w'\\-_]+[\\w])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "hex_re = re.compile(r'\\\\x[0-9a-z]{1,4}')\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False, use_unicode=True):\n",
    "        self.preserve_case = preserve_case\n",
    "        self.use_unicode = use_unicode\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        if self.use_unicode:\n",
    "            try:\n",
    "                s = str(s)\n",
    "            except UnicodeDecodeError:\n",
    "                s = str(s).encode('string_escape')\n",
    "                s = str(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        s = self.__removeHex(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        #print words #debug\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "        \n",
    "        return words\n",
    "\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, chr(entnum))\t\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, chr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s\n",
    "\n",
    "    def __removeHex(self, s):\n",
    "        return hex_re.sub(' ', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import requests\n",
    "from io import StringIO\n",
    "def LeXmo(text,document,dictionary):\n",
    "\n",
    "    '''\n",
    "      Takes text and adds if to a dictionary with 10 Keys  for each of the 10 emotions in the NRC Emotion Lexicon,\n",
    "      each dictionay contains the value of the text in that emotions divided to the text word count\n",
    "      INPUT: string\n",
    "      OUTPUT: dictionary with the text and the value of 10 emotions\n",
    "      '''\n",
    "    reponse = \"\"\n",
    "    choice = 0\n",
    "    df = pd.DataFrame()\n",
    "    emodic = {'text': text}\n",
    "    if(dictionary == \"all\"):\n",
    "        #first nrc\n",
    "        choice = 1\n",
    "    if(dictionary == \"nrc\" or choice == 1):\n",
    "        response = requests.get('https://raw.github.com/dinbav/LeXmo/master/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')\n",
    "        nrc = StringIO(response.text)\n",
    "\n",
    "\n",
    "#        emodic = {'anger': [], 'anticipation': [], 'disgust': [], 'fear': [], 'joy': [], 'negative': [],                  'positive': [], 'sadness': [], 'surprise': [], 'trust': []}\n",
    "        thisdic =  {'anger': [], 'anticipation': [], 'disgust': [], 'fear': [], 'joy': [], 'negative': [],'positive': [], 'sadness': [], 'surprise': [], 'trust': []}\n",
    "        emodic = Merge(emodic,thisdic)\n",
    "\n",
    "        lexicon = pd.read_csv(nrc,\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "        df = df.append(lexicon)\n",
    "\n",
    "    if(dictionary == \"liwc\" or choice == 1):\n",
    "        #response = requests.get('/home/kokil/feature_extraction/data/liwc2015.txt')\n",
    "        #liwc = StringIO(response.text)\n",
    "        liwc = '/home/kokil/feature_extraction/data/liwc2015.txt'\n",
    "\n",
    "\n",
    "       # emodic = {'text': text, 'PPRON': [],'BODY': [],'WE': [],'DEATH': [],'FOCUSFUTURE': [],'FEEL': [],'INTERROG': [],'NUMBER': [],'POSEMO': [],'NEGATE': [],'QUANT': [],'THEY': [],'AFFECT': [],'RELATIV': [],'HOME': [],'CONJ': [],'COGPROC': [],'SEXUAL': [],'AUXVERB': [],'SHEHE': [],'BIO': [],'DIFFER': [],'POWER': [],'NETSPEAK': [],'INFORMAL': [],'CAUSE': [],'FILLER': [],'INSIGHT': [],'LEISURE': [],'NEGEMO': [],'MOTION': [],'SEE': [],'FOCUSPAST': [],'ANGER': [],'ARTICLE': [],'NONFLU': [],'MALE': [],'WORK': [],'FRIEND': [],'FUNCTION': [],'RISK': [],'FAMILY': [],'SPACE': [],'I': [],'IPRON': [],'SOCIAL': [],'ASSENT': [],'DRIVES': [],'PERCEPT': [],'VERB': [],'HEAR': [],'FEMALE': [],'DISCREP': [],'YOU': [],'ADJ': [],'ACHIEVE': [],'RELIG': [],'TENTAT': [],'COMPARE': [],'ADVERB': [],'PRONOUN': [],'MONEY': [],'FOCUSPRESENT': [],'INGEST': [],'AFFILIATION': [],'SWEAR': [],'HEALTH': [],'SAD': [],'TIME': [],'REWARD': [],'ANX': [],'PREP': [],'CERTAIN': []}\n",
    "        thisdic = {'PPRON': [],'BODY': [],'WE': [],'DEATH': [],'FOCUSFUTURE': [],'FEEL': [],'INTERROG': [],'NUMBER': [],'POSEMO': [],'NEGATE': [],'QUANT': [],'THEY': [],'AFFECT': [],'RELATIV': [],'HOME': [],'CONJ': [],'COGPROC': [],'SEXUAL': [],'AUXVERB': [],'SHEHE': [],'BIO': [],'DIFFER': [],'POWER': [],'NETSPEAK': [],'INFORMAL': [],'CAUSE': [],'FILLER': [],'INSIGHT': [],'LEISURE': [],'NEGEMO': [],'MOTION': [],'SEE': [],'FOCUSPAST': [],'ANGER': [],'ARTICLE': [],'NONFLU': [],'MALE': [],'WORK': [],'FRIEND': [],'FUNCTION': [],'RISK': [],'FAMILY': [],'SPACE': [],'I': [],'IPRON': [],'SOCIAL': [],'ASSENT': [],'DRIVES': [],'PERCEPT': [],'VERB': [],'HEAR': [],'FEMALE': [],'DISCREP': [],'YOU': [],'ADJ': [],'ACHIEVE': [],'RELIG': [],'TENTAT': [],'COMPARE': [],'ADVERB': [],'PRONOUN': [],'MONEY': [],'FOCUSPRESENT': [],'INGEST': [],'AFFILIATION': [],'SWEAR': [],'HEALTH': [],'SAD': [],'TIME': [],'REWARD': [],'ANX': [],'PREP': [],'CERTAIN': []}\n",
    "        emodic = Merge(emodic,thisdic)\n",
    "        lexicon = pd.read_csv(liwc,\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "        df = df.append(lexicon)\n",
    "    \n",
    "    if(dictionary == \"delib\" or choice == 1):\n",
    "        delib = '/home/kokil/feature_extraction/data/dd_delib.txt'\n",
    "        #response = requests.get('/home/kokil/feature_extraction/data/dd_delib.txt')\n",
    "        #delib = StringIO(response.text)\n",
    "        \n",
    "\n",
    "\n",
    "       # emodic = {'text': text, 'EMP_RES': [],'UNCIVIL_ABUSE': [],'CONSTRUCTIVENESS': [],'JUSTIFICATION': [],'RECIPROCITY': [],'JUST_EXT': [],'RELEVANCE': [],'JUST_INT': []}\n",
    "        thisdic =  {'EMP_RES': [],'UNCIVIL_ABUSE': [],'CONSTRUCTIVENESS': [],'JUSTIFICATION': [],'RECIPROCITY': [],'JUST_EXT': [],'RELEVANCE': [],'JUST_INT': []}\n",
    "        emodic = Merge(emodic,thisdic)\n",
    "        \n",
    "\n",
    "        lexicon = pd.read_csv(delib,\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "        df = df.append(lexicon)\n",
    "    if(dictionary == \"hate\" or choice == 1):\n",
    "        hate = '/home/kokil/feature_extraction/data/incivilities.txt'\n",
    "        #response = requests.get('/home/kokil/feature_extraction/data/incivilities.txt')\n",
    "        #hate = StringIO(response.text)\n",
    "        \n",
    "\n",
    "\n",
    "        #emodic = {'text': text, 'SWEAR': [],'UNCIV': [],'OFFEN': []}\n",
    "        thisdic = {'UNCIV': [],'OFFEN': []}\n",
    "        emodic = Merge(emodic,thisdic)\n",
    "\n",
    "        lexicon = pd.read_csv(hate,           names=[\"word\", \"emotion\", \"association\"],             sep=r'\\t', engine='python')\n",
    "        df = df.append(lexicon)\n",
    "        \n",
    "    df = df.drop_duplicates(subset=['word', 'emotion'])\n",
    "    df.reset_index()\n",
    "    emolex_words = df.pivot(index='word',\n",
    "                                   columns='emotion',\n",
    "                                   values='association').reset_index()\n",
    "    emolex_words.drop(emolex_words.index[0])\n",
    "\n",
    "    categories = emolex_words.columns.drop('word')\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    rows_list = []\n",
    "    word_count = len(document)\n",
    "    for word in document:\n",
    "            word = stemmer.stem(word.lower())\n",
    "\n",
    "            emo_score = (emolex_words[emolex_words.word == word])\n",
    "            rows_list.append(emo_score)\n",
    "\n",
    "            \n",
    "    df = pd.concat(rows_list)\n",
    "    df.reset_index(drop=True)\n",
    "\n",
    "    for category in list(categories):\n",
    "        emodic[category] = df[category].sum() / word_count\n",
    "\n",
    "    return emodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def tokenize_messages(filename,col_text,col_msgid):\n",
    "    with open(filename,encoding=\"utf-8\") as corpus:\n",
    "            reader = csv.reader(corpus)\n",
    "    #splitsfile = open('C:/Users/User/Dropbox/Content Analysis/Corpus/Full Corpus/fullcorpus_split.csv','a',newline='',encoding=\"utf-8\")\n",
    "    #f_revs = csv.writer(splitsfile)\n",
    "    #f_revs.writerow([\"message_id\",\"SITE ID\",\"message\",\"Like Count\",\"postlength\"])\n",
    "    \n",
    "            rows_list = []\n",
    "            for row in reader:\n",
    "                message = row[col_text]\n",
    "                tokenizer = Tokenizer(preserve_case=True)\n",
    "                words = tokenizer.tokenize(message.lower())\n",
    "                #print(words)\n",
    "                totalGrams=0\n",
    "                freqs = dict()    \n",
    "                totalChars = 0\n",
    "                gram = '' \n",
    "                for n in range (1,4):\n",
    "                    for i in range(0,(len(words) - n)+1):\n",
    "                        totalGrams += 1\n",
    "                        gram = ' '.join(words[i:i+n])\n",
    "                        try:\n",
    "                            freqs[gram] = 1\n",
    "                        except:\n",
    "                            print(\"error\")\n",
    "                freqs[\"message_id\"]=row[col_msgid]\n",
    "                rows_list.append(freqs)\n",
    "            df = pd.DataFrame(rows_list) \n",
    "            df= df.replace(np.nan, 0)\n",
    "            print(\"Writing tokenized messages to csv...\")\n",
    "            timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "            #print timestr\n",
    "            df.to_csv(\"tokenized_messages_\"+timestr+\".csv\")\n",
    "            return df\n",
    "            \n",
    "############################\n",
    "\n",
    "def emolize_messages(filename,col_text,col_msgid,choice):\n",
    "    with open(filename,encoding=\"utf-8\") as corpus:\n",
    "            reader = csv.reader(corpus)\n",
    "    #splitsfile = open('C:/Users/User/Dropbox/Content Analysis/Corpus/Full Corpus/fullcorpus_split.csv','a',newline='',encoding=\"utf-8\")\n",
    "    #f_revs = csv.writer(splitsfile)\n",
    "    #f_revs.writerow([\"message_id\",\"SITE ID\",\"message\",\"Like Count\",\"postlength\"])\n",
    "    \n",
    "            rows_list = []\n",
    "            for row in reader:\n",
    "                message = row[col_text]\n",
    "                tokenizer = Tokenizer(preserve_case=True)\n",
    "                words = tokenizer.tokenize(message.lower())\n",
    "                emodic = LeXmo(message.lower(),words,choice)\n",
    "                print(emodic)\n",
    "                rows_list.append(emodic)\n",
    "                #print(emodic)\n",
    "            df = pd.DataFrame(rows_list) \n",
    "            df= df.replace(np.nan, 0)\n",
    "            print(\"Writing emolized messages to csv...\")\n",
    "            timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "            #print timestr\n",
    "            df.to_csv(filename+\"_\"+choice+\"_\"+timestr+\".csv\")\n",
    "            return df\n",
    "            \n",
    "############################\n",
    "def extract_counts(df,X_col):\n",
    "            vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, stop_words=STOP_WORDS, strip_accents='unicode')\n",
    "            corpus = list(df[X_col].str.lower())\n",
    "            X = vectorizer.fit_transform(corpus)\n",
    "            df = df.join(pd.DataFrame(X.toarray()).add_prefix('count_'))\n",
    "            df.to_csv(os.path.join(MODIFIED_DATA, 'counts.csv'))\n",
    "def extract_tfidf(df,X_col):\n",
    "            vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer, stop_words=S\n",
    "            TOP_WORDS, strip_accents='unicode',max_features = 10000)\n",
    "            corpus = list(df[X_col].str.lower())\n",
    "            X = vectorizer.fit_transform(corpus)\n",
    "            df = df.join(pd.DataFrame(X.toarray()).add_prefix('tfidf_'))\n",
    "            df.to_csv(os.path.join(MODIFIED_DATA, 'tfidf.csv'))\n",
    "            return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# code\n",
    "# Python code to merge dict using a single \n",
    "# expression\n",
    "def Merge(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res\n",
    "\n",
    "def extract_feats(filename, X_col):\n",
    "    df = emolize_messages( filename,1,0,\"nrc\")\n",
    "#    extract_harbingers(df, X_col)\n",
    "#    extract_politeness_feats(df, X_col)\n",
    "    df = extract_tfidf(df,X_col)\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "    #print timestr\n",
    "    df.to_csv(filename+\"_allfeats_\"+timestr+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-2b7e78bbf1c5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(lexicon)\n",
      "<ipython-input-24-2b7e78bbf1c5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(lexicon)\n",
      "<ipython-input-24-2b7e78bbf1c5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(lexicon)\n",
      "<ipython-input-24-2b7e78bbf1c5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(lexicon)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8328fd6725cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/sample.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mextract_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m##        \"C:/Users/User/Dropbox/data/msgs_jun23.csv\" --predict 1 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-a54a0d2cbfd2>\u001b[0m in \u001b[0;36mextract_feats\u001b[0;34m(filename, X_col)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memolize_messages\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mextract_harbingers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mextract_politeness_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-fdfe672b70a5>\u001b[0m in \u001b[0;36memolize_messages\u001b[0;34m(filename, col_text, col_msgid, choice)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreserve_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0memodic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeXmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memodic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mrows_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memodic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-2b7e78bbf1c5>\u001b[0m in \u001b[0;36mLeXmo\u001b[0;34m(text, document, dictionary)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     emolex_words = df.pivot(index='word',\n\u001b[0m\u001b[1;32m     81\u001b[0m                                    \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'emotion'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                                    values='association').reset_index()\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(self, index, columns, values)\u001b[0m\n\u001b[1;32m   7874\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7878\u001b[0m     _shared_docs[\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/pivot.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(data, index, columns, values)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mindexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mindexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_listlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36munstack\u001b[0;34m(self, level, fill_value)\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36munstack\u001b[0;34m(obj, level, fill_value)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_1d_only_ea_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_unstack_extension_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         unstacker = _Unstacker(\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_expanddim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, index, level, constructor)\u001b[0m\n\u001b[1;32m    138\u001b[0m             )\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_selectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36m_make_selectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Index contains duplicate entries, cannot reshape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "filename = \"data/sample.csv\"\n",
    "DATA_PATH = \"data/sample.csv\"\n",
    "OUTPUT_DIR = 'data'     # You'll get 2 directories here, one will have t\n",
    "X_col = 'text'  # Name of X column (string)\n",
    "y_col = 'label'        # Name of y column (0/1)\n",
    "MODIFIED_DATA = os.path.join(OUTPUT_DIR, 'modified_data')\n",
    "os.makedirs(MODIFIED_DATA, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "extract_feats(filename,\"text\")\n",
    "##        \"C:/Users/User/Dropbox/data/msgs_jun23.csv\" --predict 1 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
