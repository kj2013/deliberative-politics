{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/kokil/feature_extraction/data/sample2.csv\"\n",
    "OUTPUT_DIR = 'data'     # You'll get 2 directories here, one will have the results and one will have CSVs with extracted features\n",
    "\n",
    "X_col = 'text'  # Name of X column (string)\n",
    "y_col = 'label'        # Name of y column (0/1)\n",
    "\n",
    "# Only harbingers and politeness features are extracted in the last section (not liwc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "MODIFIED_DATA = os.path.join(OUTPUT_DIR, 'modified_data')\n",
    "OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'results')\n",
    "os.makedirs(MODIFIED_DATA, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "\n",
    "def is_number(tok):\n",
    "    try:\n",
    "        float(tok)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    return [tok.text if not is_number(tok.text) else '_NUM_' for tok in nlp(text)]\n",
    "\n",
    "def update_metrics(report, y_test, y_pred, minority_class):\n",
    "    report[0] += metrics.accuracy_score(y_test, y_pred)\n",
    "    report[1] += metrics.f1_score(y_test, y_pred)\n",
    "    report[2] += metrics.precision_score(y_test, y_pred)\n",
    "    report[3] += metrics.recall_score(y_test, y_pred)\n",
    "    report[4] += metrics.f1_score(y_test, y_pred, average='macro')\n",
    "    temp = classification_report(y_test, y_pred, output_dict=True)\n",
    "    key = str(minority_class)\n",
    "    if key not in temp:\n",
    "        key += '.0'\n",
    "    report[5] += temp[key]['f1-score']\n",
    "    return report\n",
    "\n",
    "def sklearn_models(df, X_col, y_col, OUTPUT_PATH, generate_Xy, folds=10):\n",
    "\n",
    "    df = df[df[y_col].notna()]\n",
    "    kfold = StratifiedKFold(folds, shuffle=True, random_state=1)\n",
    "    report = []\n",
    "    vc = dict(df[y_col].value_counts())\n",
    "    minority_class = min(vc, key=vc.get)\n",
    "\n",
    "    classifiers = {\n",
    "                 'logreg': LogisticRegression(class_weight='balanced'), \n",
    "                'knn': KNeighborsClassifier(), \n",
    "                'gaussianNB': GaussianNB(),\n",
    "                'bernoulliNB': BernoulliNB(),\n",
    "                'adaboost': AdaBoostClassifier(), \n",
    "                'gradient-boosting': GradientBoostingClassifier(),\n",
    "                'dec-tree': DecisionTreeClassifier(), \n",
    "                'linear-svc': LinearSVC(class_weight='balanced'), \n",
    "                'c-svc': SVC(class_weight='balanced')\n",
    "                }\n",
    "\n",
    "\n",
    "    for method, clf in classifiers.items():\n",
    "        running_report = [0]*6\n",
    "        for train_idx, test_idx in kfold.split(df, df[y_col]):\n",
    "            train, test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "            X_train, y_train, X_test, y_test = generate_Xy(train, test, X_col=X_col, y_col=y_col, method=method)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            running_report = update_metrics(running_report, y_test, y_pred, minority_class)\n",
    "    \n",
    "        report.append([method] + [x / folds for x in running_report])\n",
    "        print(method, 'done!')\n",
    "    report = pd.DataFrame(report, columns = ['method', 'accuracy', 'f1', 'precision', 'recall', 'macro-f1', 'minority-f1'])\n",
    "    report.to_csv(OUTPUT_PATH)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Xy(train, test, **kwargs):\n",
    "    \n",
    "    main = pd.concat([train, test])\n",
    "    vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, stop_words=STOP_WORDS, strip_accents='unicode')\n",
    "    corpus = list(main[kwargs['X_col']].str.lower())\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    main = main.join(pd.DataFrame(X.toarray()).add_prefix('count_'))\n",
    "    main.to_csv(os.path.join(MODIFIED_DATA, 'count.csv'))\n",
    "    \n",
    "    vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, stop_words=STOP_WORDS, strip_accents='unicode')\n",
    "    corpus = list(train[kwargs['X_col']].str.lower())\n",
    "    X_train = vectorizer.fit_transform(corpus)\n",
    "    X_test = vectorizer.transform(list(test[kwargs['X_col']].str.lower()))\n",
    "    X_train, y_train = csr_matrix(X_train), train[kwargs['y_col']]\n",
    "    X_test, y_test = csr_matrix(X_test), test[kwargs['y_col']]\n",
    "    non_sparse = ['gaussianNB', 'lda']\n",
    "    if(kwargs['method'] in non_sparse):\n",
    "        X_train, X_test = X_train.toarray(), X_test.toarray()\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg done!\n",
      "knn done!\n",
      "gaussianNB done!\n",
      "bernoulliNB done!\n",
      "adaboost done!\n",
      "gradient-boosting done!\n",
      "dec-tree done!\n",
      "linear-svc done!\n",
      "c-svc done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "results = sklearn_models(df, X_col, y_col, os.path.join(OUTPUT_DIR, 'count_vectorizer.csv'), generate_Xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Xy(train, test, **kwargs):\n",
    "    \n",
    "    main = pd.concat([train, test])\n",
    "    vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer, stop_words=STOP_WORDS, strip_accents='unicode')\n",
    "    corpus = list(main[kwargs['X_col']].str.lower())\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    main = main.join(pd.DataFrame(X.toarray()).add_prefix('tfidf_'))\n",
    "    main.to_csv(os.path.join(MODIFIED_DATA, 'tfidf.csv'))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer, stop_words=STOP_WORDS, strip_accents='unicode')\n",
    "    corpus = list(train[kwargs['X_col']].str.lower())\n",
    "    X_train = vectorizer.fit_transform(corpus)\n",
    "    X_test = vectorizer.transform(list(test[kwargs['X_col']].str.lower()))\n",
    "    X_train, y_train = csr_matrix(X_train), train[kwargs['y_col']]\n",
    "    X_test, y_test = csr_matrix(X_test), test[kwargs['y_col']]\n",
    "    non_sparse = ['gaussianNB', 'lda']\n",
    "    if(kwargs['method'] in non_sparse):\n",
    "        X_train, X_test = X_train.toarray(), X_test.toarray()\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg done!\n",
      "knn done!\n",
      "gaussianNB done!\n",
      "bernoulliNB done!\n",
      "adaboost done!\n",
      "gradient-boosting done!\n",
      "dec-tree done!\n",
      "linear-svc done!\n",
      "c-svc done!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "results = sklearn_models(df, X_col, y_col, os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.csv'), generate_Xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature rich prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_harbingers(df, X_col):\n",
    "\n",
    "    with open('/home/kokil/diplomacy/sbirl/diplomacy/data/2015_Diplomacy_lexicon.json') as f:\n",
    "        features = json.loads(f.readline())\n",
    "\n",
    "    for feature in features:\n",
    "        harbingers = [harbinger.encode('ascii', 'ignore').decode('ascii').lower() for harbinger in features[feature]]\n",
    "        features[feature] = harbingers\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = str(text)\n",
    "        text = text.replace('\\'', '')\n",
    "        text = text.lower()\n",
    "        text = text.replace('{html}',\"\") \n",
    "        text = re.sub(re.compile('<.*?>'), '', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub('[0-9]+', '', text)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)  \n",
    "        text = \" \".join(tokens)\n",
    "        return text\n",
    "\n",
    "    def get_feature_frequency(text, feature):\n",
    "        count = 0\n",
    "        for harbinger in features[feature]:\n",
    "            count += text.count(harbinger)\n",
    "        return count\n",
    "\n",
    "    df['clean_text'] = df.apply(lambda row: clean_text(row[X_col]), axis=1)\n",
    "    for feature in features:\n",
    "        df[feature] = df.apply(lambda row: get_feature_frequency(row['clean_text'], feature), axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, Speaker, Utterance\n",
    "from convokit import download\n",
    "from convokit import TextParser\n",
    "from convokit import PolitenessStrategies\n",
    "ps = PolitenessStrategies()\n",
    "spacy_nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "cols = list(ps.transform_utterance(\"hello, could you please help me proofread this article?\", spacy_nlp=spacy_nlp).meta['politeness_strategies'])\n",
    "\n",
    "def extract_politeness_feats(df, X_col):\n",
    "\n",
    "    def extract_politeness_helper(row):\n",
    "        utt = ps.transform_utterance(row[X_col], spacy_nlp=spacy_nlp)\n",
    "        feats = [utt.meta['politeness_strategies'][x] for x in cols]\n",
    "        return pd.Series(feats)\n",
    "\n",
    "    df[cols] = df.apply(extract_politeness_helper, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'feature_politeness_==Please=='\", \" 'feature_politeness_==Please_start=='\", \" 'feature_politeness_==HASHEDGE=='\", \" 'feature_politeness_==Indirect_(btw)=='\", \" 'feature_politeness_==Hedges=='\", \" 'feature_politeness_==Factuality=='\", \" 'feature_politeness_==Deference=='\", \" 'feature_politeness_==Gratitude=='\", \" 'feature_politeness_==Apologizing=='\", \" 'feature_politeness_==1st_person_pl.=='\", \" 'feature_politeness_==1st_person=='\", \" 'feature_politeness_==1st_person_start=='\", \" 'feature_politeness_==2nd_person=='\", \" 'feature_politeness_==2nd_person_start=='\", \" 'feature_politeness_==Indirect_(greeting)=='\", \" 'feature_politeness_==Direct_question=='\", \" 'feature_politeness_==Direct_start=='\", \" 'feature_politeness_==HASPOSITIVE=='\", \" 'feature_politeness_==HASNEGATIVE=='\", \" 'feature_politeness_==SUBJUNCTIVE=='\", \" 'feature_politeness_==INDICATIVE=='\", 'claim', 'disc_temporal_rest', 'allsubj', 'disc_expansion', 'disc_contingency', 'premise', 'disc_temporal_future', 'disc_comparison']\n"
     ]
    }
   ],
   "source": [
    "# List harbingers, liwc and politeness features\n",
    "with open('/home/kokil/diplomacy/lexica_for_featextraction/2015_Diplomacy_lexicon.json') as f:\n",
    "    harb_dict = json.loads(f.readline())\n",
    "#print(harb_dict)\n",
    "main_df = pd.read_csv('/home/kokil/diplomacy/lexica_for_featextraction/politeness_list.csv')\n",
    "X_cols = list(main_df.columns) + list(harb_dict.keys())\n",
    "print(X_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Xy(train, test, **kwargs):\n",
    "    global printed\n",
    "    X_cols_filt = [x for x in X_cols if x in list(train.columns)]\n",
    "    X_cols_nf = [x for x in X_cols if x not in list(train.columns)]\n",
    "    if not printed:\n",
    "        print('[WARNING!!!] Couldnt find', X_cols_nf)\n",
    "        printed = True\n",
    "    X_train = train[X_cols_filt].to_numpy()\n",
    "    y_train = train[kwargs['y_col']]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = test[X_cols_filt].to_numpy()\n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_test = test[kwargs['y_col']]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def extract_feats(df, X_col):\n",
    "    extract_harbingers(df, X_col)\n",
    "    extract_politeness_feats(df, X_col)\n",
    "    df.to_csv(os.path.join(MODIFIED_DATA, 'harbingers_and_politeness.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING!!!] Couldnt find [\"'feature_politeness_==Please=='\", \" 'feature_politeness_==Please_start=='\", \" 'feature_politeness_==HASHEDGE=='\", \" 'feature_politeness_==Indirect_(btw)=='\", \" 'feature_politeness_==Hedges=='\", \" 'feature_politeness_==Factuality=='\", \" 'feature_politeness_==Deference=='\", \" 'feature_politeness_==Gratitude=='\", \" 'feature_politeness_==Apologizing=='\", \" 'feature_politeness_==1st_person_pl.=='\", \" 'feature_politeness_==1st_person=='\", \" 'feature_politeness_==1st_person_start=='\", \" 'feature_politeness_==2nd_person=='\", \" 'feature_politeness_==2nd_person_start=='\", \" 'feature_politeness_==Indirect_(greeting)=='\", \" 'feature_politeness_==Direct_question=='\", \" 'feature_politeness_==Direct_start=='\", \" 'feature_politeness_==HASPOSITIVE=='\", \" 'feature_politeness_==HASNEGATIVE=='\", \" 'feature_politeness_==SUBJUNCTIVE=='\", \" 'feature_politeness_==INDICATIVE=='\"]\n",
      "logreg done!\n",
      "knn done!\n",
      "gaussianNB done!\n",
      "bernoulliNB done!\n",
      "adaboost done!\n",
      "gradient-boosting done!\n",
      "dec-tree done!\n",
      "linear-svc done!\n",
      "c-svc done!\n"
     ]
    }
   ],
   "source": [
    "printed = False\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "extract_feats(df, X_col)\n",
    "results = sklearn_models(df, X_col, y_col, os.path.join(OUTPUT_DIR, 'liwc_harbingers_politeness.csv'), generate_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplomacy",
   "language": "python",
   "name": "diplomacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
